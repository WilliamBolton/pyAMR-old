
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples\examples_general\plot_wls_basic.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__examples_examples_general_plot_wls_basic.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_examples_general_plot_wls_basic.py:


Regression - WLS basic
----------------------------

The example below loads a portion of the culture dataset and creates the 
pipeline to remove outliers, imput missing data, address the class imbalance 
and scale the data features accordingly. After this, a number of estimators 
are trained and tested (see wrappers and grids). The results such as the
estimators (pickle) and metrics (csv) re stored in the specified path.

@see core
@see xxx

.. GENERATED FROM PYTHON SOURCE LINES 14-127



.. image:: /_examples/examples_general/images/sphx_glr_plot_wls_basic_001.png
    :alt: plot wls basic
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    c:\users\kelda\desktop\repositories\virtualenvs\venvpy39-datablend\lib\site-packages\statsmodels\regression\linear_model.py:764: RuntimeWarning: divide by zero encountered in log
      llf += 0.5 * np.sum(np.log(self.weights))

    Series:
    wls-rsquared                  0.4917
    wls-rsquared_adj              0.4865
    wls-fvalue                   94.8016
    wls-fprob                        0.0
    wls-aic                          inf
    wls-bic                          inf
    wls-llf                         -inf
    wls-mse_model            136607.1819
    wls-mse_resid              1440.9791
    wls-mse_total              2806.2942
    wls-const_coef              266.7767
    wls-const_std                17.6707
    wls-const_tvalue             15.0971
    wls-const_tprob                  0.0
    wls-const_cil               231.7098
    wls-const_ciu               301.8437
    wls-x1_coef                   2.3968
    wls-x1_std                    0.2462
    wls-x1_tvalue                 9.7366
    wls-x1_tprob                     0.0
    wls-x1_cil                    1.9083
    wls-x1_ciu                    2.8853
    wls-s_dw               Jarque-Ber...
    wls-s_jb_value             Prob(JB):
    wls-s_jb_prob              Cond. No.
    wls-s_skew                 Kurtosis:
    wls-s_kurtosis                      
    wls-s_omnibus_value    Prob(Omnib...
    wls-s_omnibus_prob             Skew:
    wls-m_dw                      0.1912
    wls-m_jb_value                6.7676
    wls-m_jb_prob                 0.0339
    wls-m_skew                   -0.5861
    wls-m_kurtosis                3.5002
    wls-m_nm_value                7.2826
    wls-m_nm_prob                 0.0262
    wls-m_ks_value                0.5599
    wls-m_ks_prob                    0.0
    wls-m_shp_value               0.9345
    wls-m_shp_prob                0.0001
    wls-m_ad_value                2.9606
    wls-m_ad_nnorm                 False
    wls-missing                    raise
    wls-exog               [[1.0, 0.0...
    wls-endog              [23.525614...
    wls-trend                          c
    wls-weights            [0.1797373...
    wls-W                  <pyamr.met...
    wls-model              <statsmode...
    wls-id                 WLS(c,Sig(...
    dtype: object

    Regression line:
    [266.77674348 269.17357144 271.5703994  273.96722736 276.36405532
     278.76088328 281.15771124 283.5545392  285.95136716 288.34819512]

    Summary:
                                WLS Regression Results                            
    ==============================================================================
    Dep. Variable:                      y   R-squared:                       0.492
    Model:                            WLS   Adj. R-squared:                  0.487
    Method:                 Least Squares   F-statistic:                     94.80
    Date:                Wed, 17 Mar 2021   Prob (F-statistic):           4.51e-16
    Time:                        17:28:33   Log-Likelihood:                   -inf
    No. Observations:                 100   AIC:                               inf
    Df Residuals:                      98   BIC:                               inf
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const        266.7767     17.671     15.097      0.000     231.710     301.844
    x1             2.3968      0.246      9.737      0.000       1.908       2.885
    ==============================================================================
    Omnibus:                        7.573   Durbin-Watson:                   0.629
    Prob(Omnibus):                  0.023   Jarque-Bera (JB):                8.236
    Skew:                           0.454   Prob(JB):                       0.0163
    Kurtosis:                       4.073   Cond. No.                         232.
    Normal (N):                     7.283   Prob(N):                         0.026
    ==============================================================================






|

.. code-block:: default
   :lineno-start: 14

    # Import class.
    import sys
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.pyplot as plt
    import statsmodels.api as sm
    import statsmodels.robust.norms as norms

    # import weights.
    from pyamr.datasets.load import make_timeseries
    from pyamr.core.regression.wls import WLSWrapper
    from pyamr.metrics.weights import SigmoidA

    # ----------------------------
    # set basic configuration
    # ----------------------------
    # Matplotlib options
    mpl.rc('legend', fontsize=6)
    mpl.rc('xtick', labelsize=6)
    mpl.rc('ytick', labelsize=6)

    # Set pandas configuration.
    pd.set_option('display.max_colwidth', 14)
    pd.set_option('display.width', 150)
    pd.set_option('display.precision', 4)

    # ----------------------------
    # create data
    # ----------------------------
    # Create timeseries data
    x, y, f = make_timeseries()

    # Create method to compute weights from frequencies
    W = SigmoidA(r=200, g=0.5, offset=0.0, scale=1.0)

    # Note that the function fit will call M.weights(weights) inside and will
    # store the M converter in the instance. Therefore, the code execute is
    # equivalent to <weights=M.weights(f)> with the only difference being that
    # the weight converter is not saved.
    wls = WLSWrapper(estimator=sm.WLS).fit( \
        exog=x, endog=y, trend='c', weights=f,
        W=W, missing='raise')

    # Print series.
    print("\nSeries:")
    print(wls.as_series())

    # Print regression line.
    print("\nRegression line:")
    print(wls.line(np.arange(10)))

    # Print summary.
    print("\nSummary:")
    print(wls.as_summary())

    # -----------------
    # Save & Load
    # -----------------
    # File location
    #fname = '../../examples/saved/wls-sample.pickle'

    # Save
    #wls.save(fname=fname)

    # Load
    #wls = WLSWrapper().load(fname=fname)

    # -------------
    #  Example I
    # -------------
    # This example shows how to make predictions using the wrapper and how
    # to plot the resultin data. In addition, it compares the intervales
    # provided by get_prediction (confidence intervals) and the intervals
    # provided by wls_prediction_std (prediction intervals). 
    #
    # To Do: Implement methods to compute CI and PI (see regression).

    # Variables.
    start, end = None, 180

    # Compute predictions (exogenous?). It returns a 2D array
    # where the rows contain the time (t), the mean, the lower
    # and upper confidence (or prediction?) interval.
    preds = wls.get_prediction(start=start, end=end)


    # Create figure
    fig, ax = plt.subplots(1, 1, figsize=(11,5))

    # Plotting confidence intervals
    # -----------------------------
    # Plot truth values.
    ax.plot(x, y, color='#A6CEE3', alpha=0.5, marker='o',
                  markeredgecolor='k', markeredgewidth=0.5,
                  markersize=5, linewidth=0.75, label='Observed')

    # Plot forecasted values.
    ax.plot(preds[0,:], preds[1, :], color='#FF0000', alpha=1.00,
                    linewidth=2.0, label=wls._identifier(short=True))

    # Plot the confidence intervals.
    ax.fill_between(preds[0, :], preds[2, :],
                                 preds[3, :],
                                 color='r',
                                 alpha=0.1)

    # Legend
    plt.legend()

    # Show
    plt.show()



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.123 seconds)


.. _sphx_glr_download__examples_examples_general_plot_wls_basic.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_wls_basic.py <plot_wls_basic.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_wls_basic.ipynb <plot_wls_basic.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
