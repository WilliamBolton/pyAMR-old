
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples\examples_general\plot_wls_search.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download__examples_examples_general_plot_wls_search.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_examples_general_plot_wls_search.py:


Regression - WLS search
----------------------------

The example below loads a portion of the culture dataset and creates the 
pipeline to remove outliers, imput missing data, address the class imbalance 
and scale the data features accordingly. After this, a number of estimators 
are trained and tested (see wrappers and grids). The results such as the
estimators (pickle) and metrics (csv) re stored in the specified path.

@see core
@see xxx

.. GENERATED FROM PYTHON SOURCE LINES 14-148



.. image:: /_examples/examples_general/images/sphx_glr_plot_wls_search_001.png
    :alt: plot wls search
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    c:\users\kelda\desktop\repositories\virtualenvs\venvpy39-datablend\lib\site-packages\statsmodels\regression\linear_model.py:764: RuntimeWarning: divide by zero encountered in log
      llf += 0.5 * np.sum(np.log(self.weights))
    c:\users\kelda\desktop\repositories\virtualenvs\venvpy39-datablend\lib\site-packages\statsmodels\regression\linear_model.py:764: RuntimeWarning: divide by zero encountered in log
      llf += 0.5 * np.sum(np.log(self.weights))
    c:\users\kelda\desktop\repositories\virtualenvs\venvpy39-datablend\lib\site-packages\statsmodels\regression\linear_model.py:764: RuntimeWarning: divide by zero encountered in log
      llf += 0.5 * np.sum(np.log(self.weights))
    c:\users\kelda\desktop\repositories\virtualenvs\venvpy39-datablend\lib\site-packages\statsmodels\regression\linear_model.py:764: RuntimeWarning: divide by zero encountered in log
      llf += 0.5 * np.sum(np.log(self.weights))
    c:\users\kelda\desktop\repositories\virtualenvs\venvpy39-datablend\lib\site-packages\statsmodels\regression\linear_model.py:764: RuntimeWarning: divide by zero encountered in log
      llf += 0.5 * np.sum(np.log(self.weights))

    Grid search:
                                0              1              2              3              4              5
    wls-rsquared           0.7197         0.5407         0.5182         0.4122         0.4576          0.249
    wls-rsquare...         0.7169          0.536         0.5133         0.4062          0.452         0.2414
    wls-fvalue           251.6561       115.3835       105.3906        68.7126        82.6694        32.4986
    wls-fprob                 0.0            0.0            0.0            0.0            0.0            0.0
    wls-aic             1139.1678            inf            inf            inf            inf            inf
    wls-bic             1144.3782            inf            inf            inf            inf            inf
    wls-llf             -567.5839           -inf           -inf           -inf           -inf           -inf
    wls-mse_model    1279611.7141    176865.4511    151191.0044      66122.678     67216.8778      20819.465
    wls-mse_resid       5084.7633      1532.8493      1434.5778       962.3078       813.0802       640.6272
    wls-mse_total      17958.7729      3303.8856       2947.269      1620.4933      1483.8256       844.4538
    wls-const_coef       173.6396        259.254       268.6949       313.5658       300.0417       375.9477
    wls-const_std         14.1552        17.3111        17.3566        16.7919        16.9162        13.8522
    wls-const_t...        12.2668        14.9762        15.4808        18.6736         17.737          27.14
    wls-const_t...            0.0            0.0            0.0            0.0            0.0            0.0
    wls-const_cil         145.549       224.9007       234.2512       280.2428       266.4721       348.4585
    wls-const_ciu        201.7301       293.6074       303.1386       346.8888       333.6113        403.437
    wls-x1_coef            3.9188         2.6096         2.4829         1.8872          2.052         1.0831
    wls-x1_std              0.247         0.2429         0.2419         0.2277         0.2257           0.19
    wls-x1_tvalue         15.8637        10.7417         10.266         8.2893         9.0923         5.7008
    wls-x1_tprob              0.0            0.0            0.0            0.0            0.0            0.0
    wls-x1_cil             3.4286         2.1275          2.003         1.4354         1.6041         0.7061
    wls-x1_ciu              4.409         3.0917         2.9629          2.339         2.4998         1.4601
    wls-s_dw        Jarque-Ber...  Jarque-Ber...  Jarque-Ber...  Jarque-Ber...  Jarque-Ber...  Jarque-Ber...
    wls-s_jb_value      Prob(JB):      Prob(JB):      Prob(JB):      Prob(JB):      Prob(JB):      Prob(JB):
    wls-s_jb_prob       Cond. No.      Cond. No.      Cond. No.      Cond. No.      Cond. No.      Cond. No.
    wls-s_skew          Kurtosis:      Kurtosis:      Kurtosis:      Kurtosis:      Kurtosis:      Kurtosis:
    wls-s_kurtosis                                                                                          
    wls-s_omnib...  Prob(Omnib...  Prob(Omnib...  Prob(Omnib...  Prob(Omnib...  Prob(Omnib...  Prob(Omnib...
    wls-s_omnib...          Skew:          Skew:          Skew:          Skew:          Skew:          Skew:
    wls-m_dw                0.183         0.1337         0.1259         0.0921         0.1015         0.0595
    wls-m_jb_value         3.8296         5.9011          7.653         15.031        13.2896        19.2474
    wls-m_jb_prob          0.1474         0.0523         0.0218         0.0005         0.0013         0.0001
    wls-m_skew             0.4734        -0.5639        -0.6489        -0.9334        -0.8732        -1.0714
    wls-m_kurtosis         2.8498         3.3796         3.3901         3.3503         3.3736         3.1671
    wls-m_nm_value         3.8964         6.4576         8.0102        13.6279        12.4255        16.0975
    wls-m_nm_prob          0.1425         0.0396         0.0182         0.0011          0.002         0.0003
    wls-m_ks_value           0.56          0.574         0.5899           0.61            0.6         0.6993
    wls-m_ks_prob             0.0            0.0            0.0            0.0            0.0            0.0
    wls-m_shp_v...         0.9663         0.9519         0.9431         0.9025         0.9123         0.8681
    wls-m_shp_prob         0.0117         0.0011         0.0003            0.0            0.0            0.0
    wls-m_ad_value         1.3267         2.0868         2.4836         4.2868         3.8703         5.6004
    wls-m_ad_nnorm          False          False          False          False          False          False
    wls-exog        [[1.0, 0.0...  [[1.0, 0.0...  [[1.0, 0.0...  [[1.0, 0.0...  [[1.0, 0.0...  [[1.0, 0.0...
    wls-endog       [88.225766...  [88.225766...  [88.225766...  [88.225766...  [88.225766...  [88.225766...
    wls-trend                   c              c              c              c              c              c
    wls-weights     [1.0, 1.0,...  [0.2267212...  [0.2009592...  [0.0922659...  [0.0828947...  [6.4958872...
    wls-W           <statsmode...  <pyamr.met...  <pyamr.met...  <pyamr.met...  <pyamr.met...  <pyamr.met...
    wls-model       <statsmode...  <statsmode...  <statsmode...  <statsmode...  <statsmode...  <statsmode...
    wls-id          WLS(c,Leas...  WLS(c,Sig(...  WLS(c,Sig(...  WLS(c,Sig(...  WLS(c,Sig(...  WLS(c,Sig(...






|

.. code-block:: default
   :lineno-start: 14

    # Import class.
    import sys
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.pyplot as plt
    import statsmodels.api as sm
    import statsmodels.robust.norms as norms

    # import weights.
    from pyamr.datasets.load import make_timeseries
    from pyamr.core.regression.wls import WLSWrapper
    from pyamr.metrics.weights import SigmoidA

    # ----------------------------
    # set basic configuration
    # ----------------------------
    # Matplotlib options
    mpl.rc('legend', fontsize=6)
    mpl.rc('xtick', labelsize=6)
    mpl.rc('ytick', labelsize=6)

    # Set pandas configuration.
    pd.set_option('display.max_colwidth', 14)
    pd.set_option('display.width', 150)
    pd.set_option('display.precision', 4)

    # ----------------------------
    # create data
    # ----------------------------
    # Create timeseries data
    x, y, f = make_timeseries()

    # -----------------------------
    # Example II
    # -----------------------------
    # This example performs grid search on a number of possible configurations
    # of the WLSWrapper. In particular, it tests the effect of different 
    # objects to compute the weights from the frequencies. It presents both
    # the resulting pandas dataframe and also a figure.

    # Configuration
    # -------------
    # This variable contains the weight functions to test. Note that in 
    # the norms module there are other options such as [norms.HuberT(), 
    # norms.Hampel(), norms.TrimmedMean(), norms.TukeyBiweight(), 
    # norms.AndreWave(), norms.RamsayE()]
    w_func = [
        norms.LeastSquares(),
        SigmoidA(r=200, g=0.5, offset=0.0, scale=1.0),
        SigmoidA(r=200, g=0.5, offset=0.0, scale=1.0, percentiles=[10, 90]),
        SigmoidA(r=200, g=0.5, offset=0.0, scale=1.0, percentiles=[25, 75]),
        SigmoidA(r=200, g=0.5, offset=0.0, scale=1.0, percentiles=[25, 90]),
        SigmoidA(r=200, g=0.5, offset=0.0, scale=1.0, percentiles=[40, 50])]

    # The grid search parameters.
    grid_params = [
        # {'exog': [x], 'endog': [y], 'trend': ['c']},
        {'exog': [x], 'endog': [y], 'trend': ['c'], 'weights': [f], 'W': w_func}
    ]

    # Grid search
    # ------------
    # Perform grid search.
    summary = WLSWrapper(estimator=sm.WLS) \
        .grid_search(grid_params=grid_params)

    # Show grid results
    # ..todo: It is weird to create an WLSWrapper jut to
    #         be able to use themethod from_list_dataframe.
    #         try to implemented separately.
    print("\nGrid search:")
    print(WLSWrapper().from_list_dataframe(summary).T)

    # Prediction
    # ----------
    # Variables.
    start, end = 10, 150

    # Create figure
    fig, axes = plt.subplots(1, 3, figsize=(10, 5))

    # Plot truth values.
    axes[0].plot(x, y, color='#A6CEE3', alpha=0.5, marker='o',
                 markeredgecolor='k', markeredgewidth=0.5,
                 markersize=5, linewidth=0.75, label='Observed')

    # Plot frequencies
    axes[0].bar(x, f, color='gray', alpha=0.7, label='Frequency')

    # For each of the models in summary
    for i, model in enumerate(summary):

        # Compute predictions.
        preds = model.get_prediction(start=start, end=end)

        # Plot forecasted values.
        axes[0].plot(preds[0, :], preds[1, :],
                     linewidth=1.0,
                     label=model._identifier(short=True))

        # Plot the confidence intervals.
        axes[0].fill_between(preds[0, :],
                             preds[2, :],
                             preds[3, :],
                             alpha=0.1)

        # Plot weights assigned to each observation
        axes[1].plot(model.weights, marker='o', alpha=0.5,
                     markeredgecolor='k', markeredgewidth=0.5,
                     markersize=4, linewidth=0.00,
                     label=model._identifier(short=True))

        # Plot weights converter (W) functions.
        if model.W is not None:
            axes[2].plot(np.linspace(0, 1, 100),
                         model.W.weights(np.linspace(0, 1, 100)),
                         label=model._identifier(short=True))

    # Grid.
    axes[0].grid(linestyle='--', linewidth=0.35, alpha=0.5)
    axes[1].grid(linestyle='--', linewidth=0.35, alpha=0.5)
    axes[2].grid(linestyle='--', linewidth=0.35, alpha=0.5)

    # Legend.
    axes[0].legend(loc=0)
    axes[1].legend(loc=0)
    axes[2].legend(loc=0)

    # Tight layout
    plt.tight_layout()

    # Show.
    plt.show()


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.481 seconds)


.. _sphx_glr_download__examples_examples_general_plot_wls_search.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_wls_search.py <plot_wls_search.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_wls_search.ipynb <plot_wls_search.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
